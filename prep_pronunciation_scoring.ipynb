{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tqFES3mXuA2n",
    "outputId": "e321fe89-ccef-4b19-92e3-862c7fe858ea"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from model import GOPT\n",
    "import pickle\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5z2U2dTpuNQ"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GoPDataset(Dataset):\n",
    "    def __init__(self, mode):\n",
    "        self.mode = mode\n",
    "        dir = \"seq_data_elsa\"\n",
    "        if mode == 'train':\n",
    "            self.feat = torch.tensor(np.load(os.getcwd()+'/data/'+dir+'/tr_feat_v2.npy'), dtype=torch.float)\n",
    "            self.label = torch.tensor(np.load(os.getcwd()+'/data/'+dir+'/tr_label_v2.npy'), dtype=torch.float)\n",
    "        elif mode == 'test':\n",
    "            self.feat = torch.tensor(np.load(os.getcwd()+'/data/'+dir+'/te_feat.npy'), dtype=torch.float)\n",
    "            self.label = torch.tensor(np.load(os.getcwd()+'/data/'+dir+'/te_label.npy'), dtype=torch.float)\n",
    "\n",
    "        # normalize the GOP feature using the training set mean and std (only count the valid token features, exclude the padded tokens).\n",
    "        # self.feat = self.norm_valid(self.feat, norm_mean, norm_std)\n",
    "        self.scaler = self.load_scaler(path=\"resources/scaler.pkl\")\n",
    "\n",
    "        # normalize data set\n",
    "        tmp_feat = self.feat.reshape(-1, 84)\n",
    "        tmp_feat = self.scaler.transform(tmp_feat)\n",
    "        tmp_feat = tmp_feat.reshape(self.feat.shape).astype('float')\n",
    "        self.feat = torch.tensor(tmp_feat, dtype=torch.float)\n",
    "\n",
    "        self.label[:, :, 1:][self.label[:, :, 1:] !=-1 ] /= 50\n",
    "        \n",
    "    def load_scaler(self, path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            scaler = pickle.load(f)\n",
    "\n",
    "        return scaler\n",
    "\n",
    "    # only normalize valid tokens, not padded token\n",
    "    def norm_valid(self, feat, norm_mean, norm_std):\n",
    "        norm_feat = torch.zeros_like(feat)\n",
    "        for i in range(feat.shape[0]):\n",
    "            for j in range(feat.shape[1]):\n",
    "                if feat[i, j, 0] != 0:\n",
    "                    norm_feat[i, j, :] = (feat[i, j, :] - norm_mean) / norm_std\n",
    "                else:\n",
    "                    break\n",
    "        return norm_feat\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.feat.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.feat[idx, :]\n",
    "        phoneme_id= self.label[idx, :, 0]\n",
    "        phone_score = self.label[idx, :, 1]\n",
    "        word_score = self.label[idx, :, 2]\n",
    "        word_id = self.label[idx, :, 3]\n",
    "        utterance_score = self.label[idx, 0:1, 4]\n",
    "        \n",
    "        return {\n",
    "            \"feature\": feature, \n",
    "            \"phoneme_id\": phoneme_id, \n",
    "            \"phone_score\": phone_score,\n",
    "            \"word_score\": word_score,\n",
    "            \"word_id\": word_id,\n",
    "            \"utterance_score\": utterance_score\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tjCZLezhvxfB",
    "outputId": "e2c2efa8-289c-44a6-ae96-da2917781b5f"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "def train(audio_model, train_loader, test_loader, args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('running on ' + str(device))\n",
    "\n",
    "    # best_cum_mAP is checkpoint ensemble from the first epoch to the best epoch\n",
    "    best_epoch, best_mse = 0, 999\n",
    "    global_step, epoch = 0, 0\n",
    "    exp_dir = args.exp_dir\n",
    "\n",
    "    # if not isinstance(audio_model, nn.DataParallel):\n",
    "    #     audio_model = nn.DataParallel(audio_model)\n",
    "\n",
    "    audio_model = audio_model.to(device)\n",
    "    # Set up the optimizer\n",
    "    trainables = [p for p in audio_model.parameters() if p.requires_grad]\n",
    "    print('Total parameter number is : {:.3f} k'.format(sum(p.numel() for p in audio_model.parameters()) / 1e3))\n",
    "    print('Total trainable parameter number is : {:.3f} k'.format(sum(p.numel() for p in trainables) / 1e3))\n",
    "    optimizer = torch.optim.Adam(trainables, args.lr, weight_decay=5e-7, betas=(0.95, 0.999))\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, list(range(10, 100, 5)), gamma=0.5, last_epoch=-1)\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    print(\"current #steps=%s, #epochs=%s\" % (global_step, epoch))\n",
    "    print(\"start training...\")\n",
    "    result = np.zeros([args.n_epochs, 32])\n",
    "\n",
    "    while epoch < args.n_epochs:\n",
    "        audio_model.train()\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            audio_input = batch[\"feature\"].to(device, non_blocking=True)\n",
    "            phn_id = batch[\"phoneme_id\"].to(device, non_blocking=True)\n",
    "            phn_label = batch[\"phone_score\"].to(device, non_blocking=True)\n",
    "            word_label = batch[\"word_score\"].to(device, non_blocking=True)\n",
    "            word_id = batch[\"word_id\"].to(device, non_blocking=True)\n",
    "            utt_label = batch[\"utterance_score\"].to(device, non_blocking=True)\n",
    "\n",
    "            # warmup\n",
    "            warm_up_step = 100\n",
    "            if global_step <= warm_up_step and global_step % 5 == 0:\n",
    "                warm_lr = (global_step / warm_up_step) * args.lr\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = warm_lr\n",
    "                #print('warm-up learning rate is {:f}'.format(optimizer.param_groups[0]['lr']))\n",
    "\n",
    "            # add random noise for augmentation.\n",
    "            # noise = (torch.rand([audio_input.shape[0], audio_input.shape[1], audio_input.shape[2]]) - 1) * args.noise\n",
    "            # noise = noise.to(device, non_blocking=True)\n",
    "            # audio_input = audio_input + noise\n",
    "\n",
    "            #print(phns.shape)\n",
    "            u, p, w = audio_model(audio_input, phn_id)\n",
    "\n",
    "            # filter out the padded tokens, only calculate the loss based on the valid tokens\n",
    "            # < 0 is a flag of padded tokens\n",
    "            mask = (phn_label>=0)\n",
    "            p = p.squeeze(2)\n",
    "            p = p * mask\n",
    "            phn_label = phn_label * mask\n",
    "            \n",
    "            loss_phn = loss_fn(p, phn_label)\n",
    "\n",
    "            # avoid the 0 losses of the padded tokens impacting the performance\n",
    "            loss_phn = loss_phn * (mask.shape[0] * mask.shape[1]) / torch.sum(mask)\n",
    "\n",
    "            # utterance level loss, also mse\n",
    "            utt_preds = u\n",
    "            # print(utt_preds.shape ,utt_label[:,0:1].shape)\n",
    "            loss_utt = loss_fn(utt_preds ,utt_label[:,0:1])\n",
    "\n",
    "            # word level loss\n",
    "            word_label = word_label\n",
    "            mask = (word_label>=0)\n",
    "            word_pred = w[:, :, 0]\n",
    "            \n",
    "            word_pred = word_pred * mask\n",
    "            word_label = word_label * mask\n",
    "            \n",
    "            loss_word = loss_fn(word_pred, word_label)\n",
    "            loss_word = loss_word * (mask.shape[0] * mask.shape[1]) / torch.sum(mask)\n",
    "\n",
    "            loss = args.loss_w_phn * loss_phn + args.loss_w_utt * loss_utt + args.loss_w_word * loss_word\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "\n",
    "        print('start validation of epoch {:d}'.format(epoch))\n",
    "\n",
    "        # ensemble results\n",
    "        # don't save prediction for the training set\n",
    "        # tr_mse, tr_corr, tr_utt_mse, tr_utt_corr, tr_word_mse, tr_word_corr = validate(audio_model, train_loader, args, -1)\n",
    "        te_mse, te_corr, te_utt_mse, te_utt_corr, te_word_mse, te_word_corr = validate(audio_model, test_loader, args, best_mse)\n",
    "\n",
    "        print('Phone: Test MSE: {:.3f}, CORR: {:.3f}'.format(te_mse.item(), te_corr))\n",
    "        print('Utterance:, MSE: {:.3f}, CORR: {:.3f}'.format(te_utt_mse[0], te_utt_corr[0]))\n",
    "        print('Word:, MSE: {:.3f}, CORR: {:.3f}'.format(te_word_mse[0], te_word_corr[0]))\n",
    "\n",
    "        print('-------------------validation finished-------------------')\n",
    "\n",
    "        if te_mse < best_mse:\n",
    "            best_mse = te_mse\n",
    "            best_epoch = epoch\n",
    "\n",
    "        if best_epoch == epoch:\n",
    "            if os.path.exists(\"%s/models/\" % (exp_dir)) == False:\n",
    "                os.mkdir(\"%s/models\" % (exp_dir))\n",
    "            torch.save(audio_model.state_dict(), \"%s/models/best_audio_model.pth\" % (exp_dir))\n",
    "\n",
    "        if global_step > warm_up_step:\n",
    "            scheduler.step()\n",
    "\n",
    "        #print('Epoch-{0} lr: {1}'.format(epoch, optimizer.param_groups[0]['lr']))\n",
    "        epoch += 1\n",
    "\n",
    "def validate(audio_model, val_loader, args, best_mse):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # if not isinstance(audio_model, nn.DataParallel):\n",
    "    #     audio_model = nn.DataParallel(audio_model)\n",
    "    audio_model = audio_model.to(device)\n",
    "    audio_model.eval()\n",
    "\n",
    "    A_phn, A_phn_target = [], []\n",
    "    A_u, A_utt_target = [], []\n",
    "    A_w, A_word_target , A_word_id= [], [], []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            audio_input = batch[\"feature\"].to(device, non_blocking=True)\n",
    "            phn_id = batch[\"phoneme_id\"].to(device, non_blocking=True)\n",
    "            phn_label = batch[\"phone_score\"]\n",
    "            word_label = batch[\"word_score\"]\n",
    "            word_id = batch[\"word_id\"]\n",
    "            utt_label = batch[\"utterance_score\"]\n",
    "\n",
    "            # compute output\n",
    "            u, p, w = audio_model(audio_input, phn_id)\n",
    "            # print(u.shape, p.shape, w.shape)\n",
    "            p = p.to('cpu').detach()\n",
    "            u = u.to('cpu').detach()\n",
    "            w = w.to('cpu').detach()\n",
    "            \n",
    "            A_phn.append(p[:, :, 0])\n",
    "            A_phn_target.append(phn_label)\n",
    "            \n",
    "            A_u.append(u[:, 0:1])\n",
    "            A_utt_target.append(utt_label)\n",
    "\n",
    "            A_w.append(w[:, :, 0])\n",
    "            A_word_target.append(word_label)\n",
    "            A_word_id.append(word_id)\n",
    "            \n",
    "        index = random.randint(0, len(A_phn)-1)\n",
    "        print(\"++++++++++++++++++++++++++++++++++++++++++\")\n",
    "        print(\"## predicted phone: \", A_phn[index][0][0:6])\n",
    "        print(\"## label phone: \", A_phn_target[index][0][0:6])\n",
    "\n",
    "        print(\"## predicted word: \", A_w[index][0][0:6])\n",
    "        print(\"## label word: \", A_word_target[index][0][0:6])\n",
    "\n",
    "        print(\"## predicted utt: \", A_u[index][0])\n",
    "        print(\"## label utt: \", A_utt_target[index][0])\n",
    "        print(\"++++++++++++++++++++++++++++++++++++++++++\")\n",
    "\n",
    "        # phone level\n",
    "        A_phn, A_phn_target  = torch.vstack(A_phn), torch.vstack(A_phn_target)\n",
    "        # print(\"Phone Level: \", A_phn.shape, A_phn_target.shape)\n",
    "        # utterance level\n",
    "        A_u, A_utt_target = torch.vstack(A_u), torch.vstack(A_utt_target)\n",
    "        # print(\"Utterance Level: \", A_u.shape, A_utt_target.shape)\n",
    "        # word level\n",
    "        A_w, A_word_target, A_word_id = torch.vstack(A_w), torch.vstack(A_word_target), torch.vstack(A_word_id)\n",
    "        # print(\"Word Level: \", A_w.shape, A_word_target.shape)\n",
    "        # get the scores\n",
    "        phn_mse, phn_corr = valid_phn(A_phn, A_phn_target)\n",
    "\n",
    "        A_utt = A_u\n",
    "        utt_mse, utt_corr = valid_utt(A_utt, A_utt_target)\n",
    "\n",
    "        A_word = A_w\n",
    "        word_mse, word_corr, valid_word_pred, valid_word_target = valid_word(A_word, A_word_target, A_word_id)\n",
    "        # word_mse, word_corr, valid_word_pred, valid_word_target = 0, 0, 0 , 0\n",
    "\n",
    "        if phn_mse < best_mse:\n",
    "            print('new best phn mse {:.3f}, now saving predictions.'.format(phn_mse))\n",
    "\n",
    "            # create the directory\n",
    "            if os.path.exists(args.exp_dir + '/preds') == False:\n",
    "                os.mkdir(args.exp_dir + '/preds')\n",
    "\n",
    "            # saving the phn target, only do once\n",
    "            if os.path.exists(args.exp_dir + '/preds/phn_target.npy') == False:\n",
    "                np.save(args.exp_dir + '/preds/phn_target.npy', A_phn_target)\n",
    "                np.save(args.exp_dir + '/preds/word_target.npy', valid_word_target)\n",
    "                np.save(args.exp_dir + '/preds/utt_target.npy', A_utt_target)\n",
    "\n",
    "            np.save(args.exp_dir + '/preds/phn_pred.npy', A_phn)\n",
    "            np.save(args.exp_dir + '/preds/word_pred.npy', valid_word_pred)\n",
    "            np.save(args.exp_dir + '/preds/utt_pred.npy', A_utt)\n",
    "\n",
    "    return phn_mse, phn_corr, utt_mse, utt_corr, word_mse, word_corr\n",
    "\n",
    "def valid_phn(audio_output, target):\n",
    "    valid_token_pred = []\n",
    "    valid_token_target = []\n",
    "    # audio_output = audio_output.squeeze(2)\n",
    "    for i in range(audio_output.shape[0]):\n",
    "        for j in range(audio_output.shape[1]):\n",
    "            # only count valid tokens, not padded tokens (represented by negative values)\n",
    "            if target[i, j] >= 0:\n",
    "                valid_token_pred.append(audio_output[i, j])\n",
    "                valid_token_target.append(target[i, j])\n",
    "    valid_token_target = np.array(valid_token_target)\n",
    "    valid_token_pred = np.array(valid_token_pred)\n",
    "\n",
    "    valid_token_mse = np.mean((valid_token_target - valid_token_pred) ** 2)\n",
    "    # valid_token_mse = np.mean(np.abs(valid_token_target - valid_token_pred))\n",
    "    corr = np.corrcoef(valid_token_pred, valid_token_target)[0, 1]\n",
    "    return valid_token_mse, corr\n",
    "\n",
    "def valid_utt(audio_output, target):\n",
    "    mse = []\n",
    "    corr = []\n",
    "    target = target[:,0:1]\n",
    "    for i in range(1):\n",
    "        cur_mse = np.mean(((audio_output[:, i] - target[:, i]) ** 2).numpy())\n",
    "        # cur_mse = np.mean((np.abs(audio_output[:, i] - target[:, i])).numpy())\n",
    "        cur_corr = np.corrcoef(audio_output[:, i], target[:, i])[0, 1]\n",
    "        mse.append(cur_mse)\n",
    "        corr.append(cur_corr)\n",
    "    return mse, corr\n",
    "\n",
    "def valid_word(audio_output, target, word_id):\n",
    "    # word_id = target[:, :, -1]\n",
    "    # target = target[:, :, 0:3]\n",
    "    valid_token_pred = []\n",
    "    valid_token_target = []\n",
    "\n",
    "    # for each utterance\n",
    "    for i in range(target.shape[0]):\n",
    "        prev_w_id = 0\n",
    "        start_id = 0\n",
    "        # for each token\n",
    "        for j in range(target.shape[1]):\n",
    "            cur_w_id = word_id[i, j].int()\n",
    "            # if a new word\n",
    "            if cur_w_id != prev_w_id:\n",
    "                # average each phone belongs to the word\n",
    "                valid_token_pred.append(np.mean(audio_output[i, start_id: j].numpy(), axis=0))\n",
    "                valid_token_target.append(np.mean(target[i, start_id: j].numpy(), axis=0))\n",
    "                # sanity check, if the range indeed contains a single word\n",
    "                # if len(torch.unique(target[i, start_id: j])) != 1:\n",
    "                #     print(target[i, start_id: j])\n",
    "                # if end of the utterance\n",
    "                if cur_w_id == -1:\n",
    "                    break\n",
    "                else:\n",
    "                    prev_w_id = cur_w_id\n",
    "                    start_id = j\n",
    "\n",
    "    valid_token_pred = np.array(valid_token_pred)\n",
    "    # this rounding is to solve the precision issue in the label\n",
    "    valid_token_target = np.array(valid_token_target).round(2)\n",
    "\n",
    "    mse_list, corr_list = [], []\n",
    "    # for each (accuracy, stress, total) word score\n",
    "    valid_token_mse = np.mean((valid_token_target[:] - valid_token_pred[:]) ** 2)\n",
    "    corr = np.corrcoef(valid_token_pred[:], valid_token_target[:])[0, 1]\n",
    "    mse_list.append(valid_token_mse)\n",
    "    corr_list.append(corr)\n",
    "    return mse_list, corr_list, valid_token_pred, valid_token_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VOEATqXrwkqn",
    "outputId": "d0957691-e65d-487d-8f9c-400d8778e833"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "print(\"I am process %s, running on %s: starting (%s)\" % (os.getpid(), os.uname()[1], time.asctime()))\n",
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument(\"--exp-dir\", type=str, default=os.getcwd()+\"/exp/\", help=\"directory to dump experiments\")\n",
    "parser.add_argument('--lr', '--learning-rate', default=1e-3, type=float, metavar='LR', help='initial learning rate')\n",
    "parser.add_argument(\"--n-epochs\", type=int, default=50, help=\"number of maximum training epochs\")\n",
    "parser.add_argument(\"--goptdepth\", type=int, default=3, help=\"3 depth of gopt models\")\n",
    "parser.add_argument(\"--goptheads\", type=int, default=1, help=\"heads of gopt models\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"training batch size\")\n",
    "parser.add_argument(\"--embed_dim\", type=int, default=24, help=\"24 gopt transformer embedding dimension\")\n",
    "parser.add_argument(\"--loss_w_phn\", type=float, default=1, help=\"weight for phoneme-level loss\")\n",
    "parser.add_argument(\"--loss_w_word\", type=float, default=1, help=\"weight for word-level loss\")\n",
    "parser.add_argument(\"--loss_w_utt\", type=float, default=1, help=\"weight for utterance-level loss\")\n",
    "parser.add_argument(\"--model\", type=str, default='gopt', help=\"name of the model\")\n",
    "parser.add_argument(\"--am\", type=str, default='librispeech', help=\"name of the acoustic models\")\n",
    "parser.add_argument(\"--noise\", type=float, default=0., help=\"the scale of random noise added on the input GoP feature\")\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "if torch.cuda.is_available() == False:\n",
    "    raise ValueError('GPU is not enabled. Please go to top menu - edit - notebook settings -hardware accelerator - GPU')\n",
    "\n",
    "input_dim = 84\n",
    "\n",
    "audio_mdl = GOPT(embed_dim=args.embed_dim, num_heads=args.goptheads, depth=args.goptdepth, input_dim=input_dim)\n",
    "\n",
    "tr_dataset = GoPDataset('train')\n",
    "print(\"Num train sample: \", len(tr_dataset))\n",
    "tr_dataloader = DataLoader(tr_dataset, batch_size=25, shuffle=True, drop_last=True, num_workers=4)\n",
    "te_dataset = GoPDataset('test')\n",
    "print(\"Num test sample: \", len(te_dataset))\n",
    "te_dataloader = DataLoader(te_dataset, batch_size=256, shuffle=False, drop_last=True, num_workers=4)\n",
    "\n",
    "if os.path.exists(args.exp_dir) == False:\n",
    "  os.makedirs(args.exp_dir)\n",
    "train(audio_mdl, tr_dataloader, te_dataloader, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### v1\n",
    "# -------------------validation finished-------------------\n",
    "# start validation of epoch 36\n",
    "# Phone: Test MSE: 539.188, CORR: 0.765\n",
    "# Utterance:, MSE: 246.776, CORR: 0.789\n",
    "# Word:, MSE: 293.750, CORR: 0.766\n",
    "# -------------------validation finished-------------------\n",
    "# start validation of epoch 37\n",
    "# Phone: Test MSE: 539.483, CORR: 0.765\n",
    "# Utterance:, MSE: 247.567, CORR: 0.788\n",
    "# Word:, MSE: 293.962, CORR: 0.766\n",
    "# -------------------validation finished-------------------\n",
    "# start validation of epoch 38\n",
    "# new best phn mse 538.541, now saving predictions.\n",
    "# Phone: Test MSE: 538.541, CORR: 0.766\n",
    "# Utterance:, MSE: 246.970, CORR: 0.789\n",
    "# Word:, MSE: 293.527, CORR: 0.766"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### v2\n",
    "## predicted phone:  tensor([ 1.9180,  0.5972,  0.1705,  1.8036,  0.7993, -0.1650])\n",
    "## label phone:  tensor([ 2.,  0.,  0.,  2., -1., -1.])\n",
    "## predicted word:  tensor([0.7987, 0.8188, 0.7050, 0.7736, 0.3033, 0.1018])\n",
    "## label word:  tensor([ 0.6400,  0.6400,  0.6400,  0.6400, -1.0000, -1.0000])\n",
    "## predicted utt:  tensor([0.7410])\n",
    "## label utt:  tensor([0.6400])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "GOPT_GPU",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
